{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Nicholas A. Ondo\n",
    "### License: MIT\n",
    "\n",
    "---\n",
    "# Scraping BoxOfficeMojo\n",
    "\n",
    "I will be scraping BoxOfficeMojo using the following logic:\n",
    "\n",
    "1. Scrape the pages containing the movies per year, iterating over years and then the pages per year, to create a list of links to the movies that year.\n",
    "2. Iterate over the list of movies, scraping out the basic information I want (e.g. ).\n",
    "3. Do basic sanity checks that the data has been successfully scraped from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "def get_soup_from(url):\n",
    "    sauce = requests.get(\"http://www.boxofficemojo.com\" + url.strip())\n",
    "    soup = BeautifulSoup(sauce.text, \"lxml\")\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Scraping the Yearly List of Movies\n",
    "\n",
    "Useful notes:\n",
    "\n",
    "1. So BoxOfficeMojo (henceforth BOM) has a standardized URL for accessing yearly data, namely:\n",
    "\n",
    "```\n",
    "http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&yr=1980\n",
    "```\n",
    "\n",
    "which is easily controlled via the ```page=``` and  ```yr=``` flags.  These can be iterated over easily in Python.\n",
    "\n",
    "2. The database ranges from 1980 (for domestic) and 1990 (for international) grosses.  There's a choice on how far back to use, but for convenience sake, I will grab it all now and decide later.\n",
    "\n",
    "3. It's easiest just to look for regex patterns ```/movies/?id=``` since this is the pattern for (although it turns out we'll need to ignore the first entry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.boxofficemojo.com/yearly/chart/?page=5&view=releasedate&yr=1999\"\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the list of movies from a single given page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "print(len(soup.find_all(\"a\", href=True)))\n",
    "\n",
    "# NOTE: I chose page 5 for demonstration purposes, there aren't supposed to be any movies here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/movies/?id=hoteltransylvania3.htm']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_pattern = re.compile(r\"/movies/\\?id=\")\n",
    "\n",
    "movie_urls = []\n",
    "for tag in soup.find_all('a', href=True):\n",
    "    match = movie_pattern.search(tag[\"href\"])\n",
    "\n",
    "    if match is not None:\n",
    "        movie_urls.append(tag[\"href\"])\n",
    "        \n",
    "movie_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that without killing the zeroeth entry, the first movie is from 2018, and it's because BOM has this practice of putting the current top grossing film in theaters now at the top of the page.  So we can outright ignore the first entry on every single page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking it over, it seems like the others are totally fine, so I will wait until I scrape more to see if it is a problem on other pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping through a given year to scrape all of the movies that year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_movie_urls(year, page):\n",
    "    url = f\"http://www.boxofficemojo.com/yearly/chart/?page={page}&yr={year}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"CONNECTION FAILED: year={year} at page={page}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    movie_pattern = re.compile(r\"/movies/\\?id=\")\n",
    "    \n",
    "    movie_urls = []\n",
    "    for tag in soup.find_all('a', href=True):\n",
    "        match = movie_pattern.search(tag[\"href\"])\n",
    "\n",
    "        if match is not None:\n",
    "            movie_urls += [tag[\"href\"] + \"\\n\"]\n",
    "\n",
    "    return movie_urls[1:]\n",
    "\n",
    "\n",
    "# scrape_movie_urls(\"1980\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [year for year in range(1980,2019)]\n",
    "pages = [page for page in range(1,5)]\n",
    "\n",
    "def scrape_all_movie_urls():\n",
    "    # This is only 60 pages or so, I do not believe I will need to worry about page limiting.\n",
    "    with open(\"movie_urls.txt\", \"w\") as f:\n",
    "        for year in years:\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    urls = scrape_movie_urls(year, page)\n",
    "                except:\n",
    "                    urls = [f\"FAILED: year={year}, page={page}.  Try this again.\\n\"]\n",
    "\n",
    "                f.writelines(urls)\n",
    "\n",
    "def load_movie_urls():\n",
    "    try:\n",
    "        with open(\"movie_urls.txt\", \"r\") as f:\n",
    "            urls = f.readlines()\n",
    "    except:\n",
    "        urls = scrape_all_movies_urls()\n",
    "        \n",
    "    return urls\n",
    "        \n",
    "# movie_urls = load_movie_urls()\n",
    "# movie_urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__N.B.__ Note that there are no fails, we have grabbed all of the website URL's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_table(soup, identifier):                               \n",
    "    for table_tag in soup.find_all(\"table\", bgcolor=True):             # <table> loop\n",
    "        for td_tag in table_tag.find_all(\"td\"):                        # <td> loop\n",
    "            if identifier in td_tag.text:                              # check query condition\n",
    "                return (td_tag.text\n",
    "                        .replace(identifier, \"\")\n",
    "                        .replace(\":\", \"\").strip())\n",
    "\n",
    "\n",
    "def scrape_player_table(soup, player):\n",
    "    player_names = []\n",
    "    for some_divtag in soup.find_all(\"div\", class_=\"mp_box_content\"): # <div> loop\n",
    "        for tr_tag in some_divtag.find_all(\"tr\"):                     # <tr> loop\n",
    "            if tr_tag is not None and player in tr_tag.text.strip():  # check query condition\n",
    "                for a_tag in tr_tag.find_all(\"a\", href=True):         # <a> loop\n",
    "                    if \"*\" not in a_tag.text:                         # don't grab minor roles\n",
    "                        player_names.append(a_tag.text.strip())       # Add the value, including name\n",
    "                break\n",
    "    player_names = player_names[1:]\n",
    "    \n",
    "    if player_names != []:\n",
    "        return \",\".join(player_names)\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful checking values\n",
    "url1 = \"/movies/?id=starwars5.htm\"       # 1980\n",
    "url2 = \"/movies/?id=austinpowers2.htm\"   # 1999\n",
    "url3 = \"/movies/?id=toystory2.htm\"       # 1999\n",
    "url4 = \"/movies/?id=indianajones4.htm\"   # 2008\n",
    "url5 = \"/movies/?id=aquietplace.htm\"     # 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that then pulls all of these properties from a soup\n",
    "\n",
    "def scrape_all_movdata(soup):\n",
    "    mov_attribs = [\"Domestic Total Gross\", \"Distributor\", \"Genre\",\n",
    "                   \"MPAA Rating\", \"Release Date\", \"Runtime\", \"Production Budget\"]\n",
    "    players = [\"Director\", \"Producer\", \"Actor\"]\n",
    "    \n",
    "    vals = {}\n",
    "    vals[\"Name\"] = soup.find(\"title\").text[:-25]\n",
    "    \n",
    "    for id_text in mov_attribs:\n",
    "        vals[id_text] = [scrape_top_table(soup, id_text)]\n",
    "\n",
    "    for player in players:\n",
    "        vals[player] = scrape_player_table(soup, player)\n",
    "\n",
    "    return pd.DataFrame(vals)\n",
    "\n",
    "# soup = get_soup_from(url4)\n",
    "# df = scrape_all_movdata(soup)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_make_df(urls, dt=1.0):\n",
    "    movie_dfs = []\n",
    "    for url in urls:\n",
    "        # Grab the data at each URL\n",
    "        a_soup = get_soup_from(url)\n",
    "        movie_dfs.append(scrape_all_movdata(a_soup))\n",
    "\n",
    "        # Wait for irregular intervals\n",
    "        time.sleep(np.random.uniform(high=dt))\n",
    "\n",
    "    return pd.concat(movie_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_movieyearly_df(year):\n",
    "    pages = [page for page in range(1,5)]\n",
    "    urls = []\n",
    "    for page in pages:\n",
    "        urls += scrape_movie_urls(year, page)\n",
    "    \n",
    "    return scrape_and_make_df([url.strip() for url in urls])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    temp_df = scrape_movieyearly_df(year)\n",
    "    temp_df.to_csv(f\"movies_{year}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
